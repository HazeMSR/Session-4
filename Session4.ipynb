{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c260c87-6856-4dd5-8b9d-3295a27c0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Create the SparkSession using the postgres driver as a config\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Session-4\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-driver.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34d0bf9-d297-49e8-bbaa-5783e25d2200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36714099-d4b3-46e5-8db5-3d03a4cdceb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- inventory_id: integer (nullable = true)\n",
      " |-- film_id: short (nullable = true)\n",
      " |-- store_id: short (nullable = true)\n",
      " |-- last_update: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe using the postgres driver in order to access to db\n",
    "inv_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://3.85.128.234:8081/dvdrental\") \\\n",
    "    .option(\"dbtable\", \"inventory\") \\\n",
    "    .option(\"user\", \"myself\") \\\n",
    "    .option(\"password\", \"mysecretpassword\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "inv_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91a0da-bd59-4338-8bbb-3084a0eaccb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e490158f-51da-47ed-9cde-808851bc2b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+--------+-------------------+\n",
      "|inventory_id|film_id|store_id|        last_update|\n",
      "+------------+-------+--------+-------------------+\n",
      "|           1|      1|       1|2006-02-15 10:09:17|\n",
      "|           2|      1|       1|2006-02-15 10:09:17|\n",
      "|           3|      1|       1|2006-02-15 10:09:17|\n",
      "|           4|      1|       1|2006-02-15 10:09:17|\n",
      "|           5|      1|       2|2006-02-15 10:09:17|\n",
      "+------------+-------+--------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inv_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97f494d3-1b88-4a6e-9d28-c2ded1d523c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+------------+-----------+---------------+-----------+------+----------------+------+--------------------+--------------------+--------------------+\n",
      "|film_id|            title|         description|release_year|language_id|rental_duration|rental_rate|length|replacement_cost|rating|         last_update|    special_features|            fulltext|\n",
      "+-------+-----------------+--------------------+------------+-----------+---------------+-----------+------+----------------+------+--------------------+--------------------+--------------------+\n",
      "|    133|  Chamber Italian|A Fateful Reflect...|        2006|          1|              7|       4.99|   117|           14.99| NC-17|2013-05-26 14:50:...|          [Trailers]|'chamber':1 'fate...|\n",
      "|    384| Grosse Wonderful|A Epic Drama of a...|        2006|          1|              5|       4.99|    49|           19.99|     R|2013-05-26 14:50:...| [Behind the Scenes]|'australia':18 'c...|\n",
      "|      8|  Airport Pollock|A Epic Tale of a ...|        2006|          1|              6|       4.99|    54|           15.99|     R|2013-05-26 14:50:...|          [Trailers]|'airport':1 'anci...|\n",
      "|     98|Bright Encounters|A Fateful Yarn of...|        2006|          1|              4|       4.99|    73|           12.99| PG-13|2013-05-26 14:50:...|          [Trailers]|'boat':20 'bright...|\n",
      "|      1| Academy Dinosaur|A Epic Drama of a...|        2006|          1|              6|       0.99|    86|           20.99|    PG|2013-05-26 14:50:...|[Deleted Scenes, ...|'academi':1 'batt...|\n",
      "+-------+-----------------+--------------------+------------+-----------+---------------+-----------+------+----------------+------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "film_df = spark.read \\\n",
    "    .format(\"jdbc\") \\\n",
    "    .option(\"url\", \"jdbc:postgresql://3.85.128.234:8081/dvdrental\") \\\n",
    "    .option(\"dbtable\", \"film\") \\\n",
    "    .option(\"user\", \"myself\") \\\n",
    "    .option(\"password\", \"mysecretpassword\") \\\n",
    "    .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "    .load()\n",
    "\n",
    "film_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19166fb8-eb3c-4259-ab32-a77366bfd642",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a dataframe with data in order to create a parquet file\n",
    "parquet_data = [(\"James \",\"\",\"Smith\",\"36636\",\"M\",3000),\n",
    "              (\"Michael \",\"Rose\",\"\",\"40288\",\"M\",4000),\n",
    "              (\"Robert \",\"\",\"Williams\",\"42114\",\"M\",4000),\n",
    "              (\"Maria \",\"Anne\",\"Jones\",\"39192\",\"F\",4000),\n",
    "              (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",-1)]\n",
    "\n",
    "parquet_columns = [\"firstname\",\"middlename\",\"lastname\",\"dob\",\"gender\",\"salary\"]\n",
    "\n",
    "parquet_df = spark.createDataFrame(parquet_data, parquet_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef0a80-13b2-4d52-9fb3-e726ded44032",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4ff9d411-619b-4549-8255-f36a6b2174dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the content of your dataframe in a parquet file\n",
    "parquet_df.write.parquet(\"people.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad96cea-04fd-4f10-a7b0-a23c4497955a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "df116144-014c-4728-9952-80f544084db4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Read your parquet file and load it into a dataframe\n",
    "people_df = spark.read.parquet(\"people.parquet\")\n",
    "people_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ef8ea3-fe13-4145-8b58-5c4bf4181e1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01eba8-18e0-4589-af71-224f2ba9a54a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e822894d-e9e9-49c3-b888-bf8873213fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Append in order to add more rows to the parquet\n",
    "\n",
    "male_people_df = people_df.filter('gender == \"M\"')\n",
    "male_people_df.write.mode('append').parquet(\"people.parquet\")\n",
    "m_df = spark.read.parquet(\"people.parquet\")\n",
    "m_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1170862f-a7fc-4d4a-a138-f377e1e35833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We can create temp tables\n",
    "\n",
    "people_df.createOrReplaceTempView(\"ParquetTable\")\n",
    "sql_context = spark.sql(\"select * from ParquetTable where salary >= 4000 \")\n",
    "sql_context.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0737e20e-0e7f-4e50-b41d-4ffb81431620",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+------+\n",
      "|firstname|middlename|lastname|  dob|gender|salary|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   Maria |      Anne|   Jones|39192|     F|  4000|\n",
      "|      Jen|      Mary|   Brown|     |     F|    -1|\n",
      "|  Robert |          |Williams|42114|     M|  4000|\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "|   James |          |   Smith|36636|     M|  3000|\n",
      "| Michael |      Rose|        |40288|     M|  4000|\n",
      "+---------+----------+--------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Also we can create the temp table directly into the parquet file\n",
    "\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON USING parquet OPTIONS (path \\\"people.parquet\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "69be7b17-738a-4e0b-8830-410c699f618e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can partition our tables in order to improve our query speed\n",
    "people_df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(\"people2.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c363beae-fdb5-47e0-aea2-3cdffd4a4d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|dob  |salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|Robert   |          |Williams|42114|4000  |\n",
      "|Michael  |Rose      |        |40288|4000  |\n",
      "|James    |          |Smith   |36636|3000  |\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retrieving from a parquet partition\n",
    "male2_df = spark.read.parquet(\"people2.parquet/gender=M\")\n",
    "male2_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "19f86f7d-94f9-45e6-b02e-ef503fa9e8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+--------+-----+------+\n",
      "|firstname|middlename|lastname|  dob|salary|\n",
      "+---------+----------+--------+-----+------+\n",
      "|   Maria |      Anne|   Jones|39192|  4000|\n",
      "|      Jen|      Mary|   Brown|     |    -1|\n",
      "+---------+----------+--------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating a temp table from a partitioned parquet file\n",
    "\n",
    "spark.sql(\"CREATE TEMPORARY VIEW PERSON2 USING parquet OPTIONS (path \\\"people2.parquet/gender=F\\\")\")\n",
    "spark.sql(\"SELECT * FROM PERSON2\" ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "3afc3db6-ccfd-412b-b1b1-fd003778ef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Name  Age\n",
      "0   Scott   50\n",
      "1    Jeff   45\n",
      "2  Thomas   54\n",
      "3     Ann   34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd    \n",
    "data = [['Scott', 50], ['Jeff', 45], ['Thomas', 54],['Ann',34]] \n",
    " \n",
    "# Create the pandas DataFrame \n",
    "pandas_df = pd.DataFrame(data, columns = ['Name', 'Age']) \n",
    "  \n",
    "# print dataframe. \n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "179f5193-6c35-468d-bad7-28f4cdc9ab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+\n",
      "|  Name|Age|\n",
      "+------+---+\n",
      "| Scott| 50|\n",
      "|  Jeff| 45|\n",
      "|Thomas| 54|\n",
      "|   Ann| 34|\n",
      "+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create PySpark DataFrame from Pandas\n",
    "spark_pandas_df = spark.createDataFrame(pandas_df) \n",
    "spark_pandas_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d0683abe-1209-4fed-80a3-c5f66aaab365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Create User defined Custom Schema using StructType\n",
    "my_schema = StructType([ StructField(\"First Name\", StringType(), True)\\\n",
    "                       ,StructField(\"Age\", IntegerType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "181e3570-951d-4362-ad4f-9b8529f4fb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|First Name|Age|\n",
      "+----------+---+\n",
      "|     Scott| 50|\n",
      "|      Jeff| 45|\n",
      "|    Thomas| 54|\n",
      "|       Ann| 34|\n",
      "+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a new dataframe using your new schema as paramater\n",
    "pandas_schema_df = spark.createDataFrame(pandas_df,schema=my_schema)\n",
    "pandas_schema_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3c946717-ba76-453f-be8c-5a6a9ded72f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pandas_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e6108262-616c-4388-b493-8f4afd130e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_pandas_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3aad7432-98ba-4e0d-a06d-c2f373fcd737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Converting pandas df using apache arrow\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\",\"true\")\n",
    "arrow_df = spark.createDataFrame(pandas_df) \n",
    "arrow_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "655d25d0-ecbc-424c-9ffa-4aa8e958450e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- first_name: string (nullable = true)\n",
      " |-- middle_name: string (nullable = true)\n",
      " |-- last_name: string (nullable = true)\n",
      " |-- dob: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- salary: long (nullable = true)\n",
      "\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|first_name|middle_name|last_name|dob  |gender|salary|\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "|James     |           |Smith    |36636|M     |60000 |\n",
      "|Michael   |Rose       |         |40288|M     |70000 |\n",
      "|Robert    |           |Williams |42114|      |400000|\n",
      "|Maria     |Anne       |Jones    |39192|F     |500000|\n",
      "|Jen       |Mary       |Brown    |     |F     |0     |\n",
      "+----------+-----------+---------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now convert a Spark DF to a Pandas DF\n",
    "# First we create a new spark df\n",
    "data = [(\"James\",\"\",\"Smith\",\"36636\",\"M\",60000),\n",
    "        (\"Michael\",\"Rose\",\"\",\"40288\",\"M\",70000),\n",
    "        (\"Robert\",\"\",\"Williams\",\"42114\",\"\",400000),\n",
    "        (\"Maria\",\"Anne\",\"Jones\",\"39192\",\"F\",500000),\n",
    "        (\"Jen\",\"Mary\",\"Brown\",\"\",\"F\",0)]\n",
    "\n",
    "columns = [\"first_name\",\"middle_name\",\"last_name\",\"dob\",\"gender\",\"salary\"]\n",
    "pyspark_df = spark.createDataFrame(data = data, schema = columns)\n",
    "pyspark_df.printSchema()\n",
    "pyspark_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "19145caa-2429-4b41-9106-b06ee894918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  first_name middle_name last_name    dob gender  salary\n",
      "0      James                 Smith  36636      M   60000\n",
      "1    Michael        Rose            40288      M   70000\n",
      "2     Robert              Williams  42114         400000\n",
      "3      Maria        Anne     Jones  39192      F  500000\n",
      "4        Jen        Mary     Brown             F       0\n"
     ]
    }
   ],
   "source": [
    "# Then use toPandas method\n",
    "pandasDF = pyspark_df.toPandas()\n",
    "print(pandasDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5d983d96-cce8-4b70-a0a0-0ba1b71a7034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nested structure elements\n",
    "from pyspark.sql.types import StructType, StructField, StringType,IntegerType\n",
    "data_struct = [((\"James\",\"\",\"Smith\"),\"36636\",\"M\",\"3000\"), \\\n",
    "      ((\"Michael\",\"Rose\",\"\"),\"40288\",\"M\",\"4000\"), \\\n",
    "      ((\"Robert\",\"\",\"Williams\"),\"42114\",\"M\",\"4000\"), \\\n",
    "      ((\"Maria\",\"Anne\",\"Jones\"),\"39192\",\"F\",\"4000\"), \\\n",
    "      ((\"Jen\",\"Mary\",\"Brown\"),\"\",\"F\",\"-1\") \\\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f6763aa1-5cb7-4400-9086-7f9dde5c12be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------+------+\n",
      "|                name|  dob|gender|salary|\n",
      "+--------------------+-----+------+------+\n",
      "|    {James, , Smith}|36636|     M|  3000|\n",
      "|   {Michael, Rose, }|40288|     M|  4000|\n",
      "|{Robert, , Williams}|42114|     M|  4000|\n",
      "|{Maria, Anne, Jones}|39192|     F|  4000|\n",
      "|  {Jen, Mary, Brown}|     |     F|    -1|\n",
      "+--------------------+-----+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create the nested schema struct\n",
    "schema_struct = StructType([\n",
    "        StructField('name', StructType([\n",
    "             StructField('firstname', StringType(), True),\n",
    "             StructField('middlename', StringType(), True),\n",
    "             StructField('lastname', StringType(), True)\n",
    "             ])),\n",
    "          StructField('dob', StringType(), True),\n",
    "         StructField('gender', StringType(), True),\n",
    "         StructField('salary', StringType(), True)\n",
    "         ])\n",
    "nested_df = spark.createDataFrame(data=data_struct, schema = schema_struct)\n",
    "nested_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "dc949f6c-d863-49b9-8be3-b6cebd049d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                name    dob gender salary\n",
      "0  {'firstname': 'James', 'middlename': '', 'last...  36636      M   3000\n",
      "1  {'firstname': 'Michael', 'middlename': 'Rose',...  40288      M   4000\n",
      "2  {'firstname': 'Robert', 'middlename': '', 'las...  42114      M   4000\n",
      "3  {'firstname': 'Maria', 'middlename': 'Anne', '...  39192      F   4000\n",
      "4  {'firstname': 'Jen', 'middlename': 'Mary', 'la...             F     -1\n"
     ]
    }
   ],
   "source": [
    "# Convert to a Pandas DF\n",
    "pandas_df2 = nested_df.toPandas()\n",
    "print(pandas_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10409206-3260-4b83-88c7-080dc4996bd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/conda/feedstock_root/build_artifacts/arrow-cpp-ext_1644752432717/work/cpp/src/plasma/store.cc:1274: Allowing the Plasma store to use up to 1GB of memory.\n",
      "/home/conda/feedstock_root/build_artifacts/arrow-cpp-ext_1644752432717/work/cpp/src/plasma/store.cc:1297: Starting object store with directory /dev/shm and huge page support disabled\n",
      "/home/conda/feedstock_root/build_artifacts/arrow-cpp-ext_1644752432717/work/cpp/src/plasma/store.cc:1315: System memory request exceeds memory available in /dev/shm. The request is for 1000000000 bytes, and the amount available is 60390604 bytes. You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you may need to pass an argument with the flag '--shm-size' to 'docker run'.\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "# !pip install pyarrow\n",
    "\n",
    "# Pyarrow + Plasma\n",
    "\n",
    "# The -m flag specifies the size of the store in bytes, and the -s flag specifies the socket that the store will listen at\n",
    "!plasma_store -m 1000000000 -s /tmp/plasma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799c91f5-e7c6-46f1-90e3-b5ccd9631553",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
